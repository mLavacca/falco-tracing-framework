
# Falco performance report
### Mattia Lavacca

## abstract
The effectiveness of Falco relies on the assumption that it is able to detect all the events in the system, analyze them and produce an output accordingly, therefore a major requirement that this kind of system is required to satisfy, is the capacity to detect all the events, without missing a single one of them. In order to ensure this kind of reliability, it is necessary that Falco is efficient and not affected by performance constraints that can inficiate its effectiveness under both low and high load conditions.

## Introduction
The analysis of the system has been divided into two different parts:
*  stack traces analysis: this kind of analysis traces all the possible branches in the execution flow of Falco and creates a stack trace for each of them, producing a profiling graph;
*   rules checking analysis: all the rules that are loaded into Falco are traced separately and they are profiled thanks to the measurement of the main metrics (counter, latency).

## The analysis system
The analysis system is composed by some different software entities:
*   Falco-plugin: some `C++` modules that provide functions for tracing operations are built with Falco. A `C` wrapper is also provided to allow `libscap` to be traced.
*   Falco-instrumentation: function calls to insert in Falco (whose build is conditioned by macros) allow the insertion of tracing points in Falco.
*   Falco-tracer: the tracer (written in `go`) that gets a configuration file (written in `yaml`), and performs the following operations:
	*   record: launches sysdig with `-w` flag (write result on disk) and lets it run for n seconds;
	*   offline-report: launches Falco n times with `-e` flag (get events from the `.scap` file produced in the previous phase); at the end of each iteration, it loads the metrics written on file by the Falco plugin, keeping up to date a data structure of the average metrics. Last, it creates many different output files needed by the metrics graphic representation tools.

## The output format of the system
In order to have a meaningful graphical representation of the system, Falco-tracer produces three different output files that will be used by different tools to produce plots and graphs:

*   custom `.json` file representing the system’s stacktrace tree and the rule metrics;
*   `.dot` file ([dot](https://www.graphviz.org/) flowchart) representing in the dot format all the possible Falco’s stacktraces;
* `.folded` file ([flamegraph](https://github.com/brendangregg/FlameGraph)) representing a flamegraph of Falco.

## Falco instrumentation
Falco has been instrumented in a static way, by calling some function calls (defined in the Falco plugin) in the places where the code metrics must be traced (for both rules and stacktraces). In order to avoid the creation of many new events that would have been pushed into the Falco internal buffers, all the analysis operations have been done without the usage of dynamic memory allocation and system calls in general.
Since insertions and lookups in `C++` data structures would have introduced too much overhead, the coupling of each function with its corresponding metric has been implemented by using direct access array (statically allocated), ensuring the affection of the lowest possible overhead.
Since every tracing function call introduces a little overhead to the system, and the rules checking loop (in which the rules metrics are measured) is nested in a monitored function call inside the main loop of Falco, in order to reduce as much as possible the overhead of the tracing, the rules metrics measurement functions have been separated by the stacktraces metrics measurement functions, by creating two different Falco builds (all the tracing functions are present in the same version of the code, but the building of them is conditioned by two different mutually exclusive macros).

## Rules loading mechanism
In order to acquire the rules, Falco uses a `lua` engine during two different moments of its lifecycle:
*   during startup, when the `lua_engine` is used for getting general information about the rules; in that moment all the rules (and the tags) are being associated to an integer number. Falco will refer to the rules (and the tags) by means of that integer number;  
*  whenever Falco needs rule name, output format, etc. due to a rule triggering; this happens when the function `falco_outputs::handle_event()` is being called.

## Falco Rules check
The main cycle of Falco performs some operations, including the main one, the rules matching: every events taken by Falco from the lower layer must be compared against a set of rules to check if there is a matching. If that happens, Falco produces an output accordingly.
The rules matching works in the following way:
1.  every rule loaded into Falco has one or more tags associated
2.  every event caught by Sysdig is tagged with one of those tags;
3.  when Falco must figure out whether the current event matches some rules, all the rules tagged in the same way of the event are checked.
4.  Every rule is a boolean expression composed by a variable number of boolean operands. The bigger the boolean expression, the bigger the overhead introduced by the single rule checking in the worst case.
5.  The function that checks whether an event matches a rule is wrapper to a recursive function, that brings a non negligible overhead to the system.

## The analysis principle
For the creation of the stacktraces, the analysis relies on the fact that the Falco core is composed by a forever loop taking one event from the lower layer (`sinsp::next()`) each iteration; then it performs some checks, and finally compares that event against a ruleset. The result got from the ruleset comparison tells whether the current event has broken some rules and which ones. After this sequence of operations, the current iteration ends up and a new iteration starts. Every iteration can end up in three different ways:
-   `SCAP_TIMEOUT`: the lower buffer is empty, there is no event to process;
-   `RULES_UNBROKEN`: the current event has not triggered any rules of the ruleset;
-   `RULES_BROKEN`: the current event has triggered one or more rule of the ruleset.

Each possible branch has its own metrics and calls different functions, therefore all the metrics must be distinguished based on them branch belonging. All the branches are labelled with the occurrences counter.


## Analysis results
Thanks to the analysis performed by the tracer framework, it is possible to trace the variation of Falco performance with the variation of the operative conditions. By observing the `.dot` graph above (the latency values are in number of clock cycles), it is possible to notice how the metrics drastically change with the type of the flowchart. As expected, indeed, the `SCAP_TIMEOUT` branch is the lightest in terms of latency, then, the overhead introduced by the functions increases with the type of event, up to the branches that are relative to the complete check of the corresponding ruleset tag, ending with the matching against one or more rules (`RULES_BROKEN` branch).
The major difference that it is possible to notice is the overhead variation of the `process_sinsp_event()` function between the iterations in which no rules are matched than the ones belonging to the `RULES_BROKEN` branch. Indeed, this difference in terms of computation overhead is brought by the `falco_sinsp_ruleset::run()`, the function that iterates over the whole filterset, by calling a recursive function devoted to check whether the current event matches some rules. That function is the core of Falco, indeed Its latency is the most variable parameter of the system, depending on which flow branch it belongs. This variation is strictly related to the overhead that every rule checking brings into the system depending on the fact that the event matches some rules or not: every rule is a boolean expression generally composed by a set of comparison operations put in logical AND, therefore whether the event doesn’t match one boolean operand of the rule, the execution ends up, without checking all the subsequent operands of the expression. It is possible to observe it in the example below:


This is a rule that check whether there has been a write operation under monitored bin: the condition field is the boolean expression that identifies which system event the rule matches and is composed by a set of boolean operands in AND condition. If anyone of them doesn’t match the specified condition, the execution of the rule is stopped and the control is given back to the calling function that will iterate over the new rule to be checked.
This behaviour explain extensively the reason why the latency of `falco_sinsp_ruleset::run()` changes considerably between the `RULES_UNBROKEN` branch and the `RULES_BROKEN` branch and by analyzing the latency distribution plot of the rules checking it is also possible to observe this impact on the rules checking:

This first plot shows the latency distribution of the rules checking occurrences that belong to the `UNBROKEN_RULES` branch: considering the write below etc rule, it is possible to observe that its overhead is around 300 clock cycles, instead of the data that it is possible to observe in the following plot, that describe the `BROKEN_RULE` branch in which some rules have been broken.

In this case, it is possible to see that the overhead brought in by the rule checker when the rule write below etc has increased hugely, from 300 to 34000 clock cycles. This behaviour, as said before is related to the fact that the overhead introduced by the rule checker varies heavily depending on which stacktrace branch the operation is located in.
The last important part of the analysis concerns the output operation that happens only in `RULES_BROKEN` branch (`falco_outputs::handle_events()`), indeed it is possible to see that its contribute in terms of latency is almost double the contribute of the rules analysis function. `falco_outputs::handle_event()` manages the lua engine for getting details about the broken rule and redirects them to the correct output. It brings in a huge overhead to the system (but only in case some rules are being triggered).

## Possible performance improvements
As it is possible to see in the .dot graph above, the cardinality of the `SCAP_TIMEOUT` branch is the highest, therefore, the greatest part of the iterations ends up immediately due to the emptiness of the sinsp buffers. This fact is very meaningful, because the first goal of Falco is to detect every event in the system and analyze it to eventually create an alert message. In normal conditions, the speed of buffer emptying is very higher than the speed of buffer filling, therefore Falco has not huge performance constraints that affect its optimal work. In order to get event dropping (buffer emptying lower that buffer filling) it is necessary to trigger this condition by producing a high rate of triggering event: on an average PC Falco starts dropping events when the ratio of the breaking events is around 1M per second (depending on the rule that is being broken).
In order to increase the ratio in which Falco starts to drop events, some different performance improvements can be done:
1.  to implement a different mechanism for checking rules matching (`falco_sinsp_ruleset::run()`) or optimize the current one;
2.  to change rules tagging: since every event is tagged with a tag that identifies that type of event, and all the rules tagged in the same way are checked for all the events tagged in the same way, by increasing the discretization of the tags, and making them more specific, at every iteration the number of the rules checked would be lower, leading to a decrease of the overhead brought by the `falco_sinsp_ruleset::run()` function. This improvement would affect all the iterations, both the ones that don’t match any rules and those that match some rules; 
3.  to improve the `falco_output::handle_event()` performances; this could be achieved by storing all the rules informations in a data structure in Falco at the first call of the lua engine, during the system startup (since every rule is associated to an incremental integer, an array with direct access is allowed to be used). By accessing all the needed informations through that data structure, instead of invoking the lua_engine every time an event matches a rule, the `falco_output::handle_event()` overhead would decrease heavily. This kind of improvement would affect only the iterations in which some rules were matched.